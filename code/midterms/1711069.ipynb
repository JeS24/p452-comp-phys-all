{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P452 - Computational Physics - MidTerms\n",
    "##### Name: Jyotirmaya Shivottam\n",
    "##### Roll: 1711069"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_reg(x, y, yerr, ret_all=False):\n",
    "    \"\"\"\n",
    "    Linear Regression Module\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: array_like\n",
    "    y: array_like\n",
    "    yerr: array_like\n",
    "    ret_all: bool\n",
    "\n",
    "    \"\"\"\n",
    "    # Calculating relevant quantities\n",
    "    S =  np.sum(1 / (yerr**2))\n",
    "    Sx = np.sum(x / (yerr**2))\n",
    "    Sy = np.sum(y / (yerr**2))\n",
    "    Sxx = np.sum((x**2) / (yerr**2))\n",
    "    Syy = np.sum((y**2) / (yerr**2))\n",
    "    Sxy = np.sum((x*y) / (yerr**2))\n",
    "\n",
    "    delta = S * Sxx - Sx ** 2\n",
    "    c = (Sxx * Sy - Sx * Sxy) / delta\n",
    "    m = (S * Sxy - Sx * Sy) / delta\n",
    "    yfit = m * x + c\n",
    "\n",
    "    omega_2m = S / delta\n",
    "    omega_2c = Sxx / delta\n",
    "    \n",
    "    cov = -Sx / delta\n",
    "    r2 = Sxy / (Sxx * Syy)\n",
    "\n",
    "    if ret_all:\n",
    "        return yfit, m, c, omega_2m, omega_2c, cov, r2\n",
    "    else: # Returning only the things required for fitting\n",
    "        return yfit, m, c\n",
    "\n",
    "def chi_sq(yexp, yfit, yerr):\n",
    "    \"\"\"\n",
    "    Calculates Goodness-of-fit (Chi-Square)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    yexp: array_like\n",
    "    yfit: array_like\n",
    "    yerr: array_like\n",
    "\n",
    "    \"\"\"\n",
    "    chi_sq = np.sum(((yexp - yfit) / yerr) ** 2)\n",
    "    chi_sqn = chi_sq / (yexp.shape[0] - 2)\n",
    "\n",
    "    return chi_sq, chi_sqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1., 106.,  10.],\n",
       "       [ 15.,  80.,   9.],\n",
       "       [ 30.,  98.,  10.],\n",
       "       [ 45.,  75.,   9.],\n",
       "       [ 60.,  74.,   8.],\n",
       "       [ 75.,  73.,   8.],\n",
       "       [ 90.,  49.,   7.],\n",
       "       [105.,  38.,   6.],\n",
       "       [120.,  37.,   6.],\n",
       "       [135.,  22.,   5.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from LibPython.Library import Statistics\n",
    "# s = Statistics()\n",
    "\n",
    "dat = np.genfromtxt(\"msfit.txt\")\n",
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = dat[:, 0]\n",
    "counts = dat[:, 1]\n",
    "std_count = dat[:, 2]\n",
    "\n",
    "counts = np.log(counts)\n",
    "std_count = 1 / std_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit, m, _, sig2m, _, _, _ = lin_reg(time, counts, std_count, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Half life: 74.8766130697403\n",
      "Error: 0.0010146470187917449\n"
     ]
    }
   ],
   "source": [
    "halflife = np.log(2) / (-m)\n",
    "error = np.sqrt(sig2m)\n",
    "\n",
    "print(f\"Half life: {halflife}\\nError: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained value: 15.17084059456294 < 16.919 (for DoF = 10 - 1 = 9). So, the null hypothesis is satisfied.\n"
     ]
    }
   ],
   "source": [
    "chi_sq_, chi_sqn = chi_sq(counts, fit, std_count)\n",
    "print(f\"Obtained value: {chi_sq_} < 16.919 (for DoF = 10 - 1 = 9). So, the null hypothesis is satisfied.\") # Source: https://people.richland.edu/james/lecture/m170/tbl-chi.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2., -1.,  0.,  0.,  0.],\n",
       "       [-1.,  2., -1.,  0.,  0.],\n",
       "       [ 0., -1.,  2., -1.,  0.],\n",
       "       [ 0.,  0., -1.,  2., -1.],\n",
       "       [ 0.,  0.,  0., -1.,  2.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimat = np.loadtxt(\"./mstrimat.txt\", delimiter=\"   \")\n",
    "trimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def norm(x):\n",
    "    \"\"\"\n",
    "    Returns norm of a vector, x\n",
    "\n",
    "    \"\"\"\n",
    "    return np.sqrt(x @ x)\n",
    "\n",
    "def power(A, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Power Iterator with Matrix Deflation\n",
    "    \n",
    "    \"\"\"\n",
    "    N = A.shape[0]\n",
    "    eigs = []\n",
    "    vecs = []\n",
    "    Ac = np.copy(A)\n",
    "    lamb = 0\n",
    "\n",
    "    for i in range(N):\n",
    "        # Initial guess\n",
    "        x = np.ones(N)\n",
    "        while True:\n",
    "            x_1 = Ac.dot(x) # y_n = A * x_(n-1)\n",
    "            x_norm = norm(x_1)\n",
    "            x_1 = x_1 / x_norm # x_n = y_n / ||y_n||\n",
    "\n",
    "            if(abs(lamb - x_norm) <= eps): # If precision is reached, it returns eigenvalue\n",
    "                break\n",
    "\n",
    "            lamb = x_norm\n",
    "            x = x_1\n",
    "\n",
    "        eigs.append(lamb)\n",
    "\n",
    "        # Matrix Deflation: A - Lambda * norm[V] * norm[V]^T\n",
    "        v = x_1 / norm(x_1)\n",
    "        R = v * v.T\n",
    "        R = eigs[i] * R\n",
    "        vecs.append(x_1)\n",
    "        Ac = Ac - R\n",
    "\n",
    "    return eigs, vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two eigenvalues and eigenvectors:\n",
      "Eigenvalue: 3.7320508074913867\n",
      "Eigenvector: [ 0.2886764 -0.5        0.577349  -0.5        0.2886764]\n",
      "\n",
      "Eigenvalue: 3.7347504708527888\n",
      "Eigenvector: [ 0.29513701 -0.4945395   0.58021519 -0.4945395   0.29513701]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "e, v = power(trimat)\n",
    "print(\"First two eigenvalues and eigenvectors:\")\n",
    "for i in range(2):\n",
    "    print(f\"Eigenvalue: {e[i]}\\nEigenvector: {v[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected first two eigenvalues and eigenvectors:\n",
      "Eigenvalue: 3.7320508075688776\n",
      "Eigenvector: [0.28867513 0.5        0.57735027 0.5        0.28867513]\n",
      "\n",
      "Eigenvalue: 3.0\n",
      "Eigenvector: [ 5.00000000e-01  5.00000000e-01  7.07050159e-17 -5.00000000e-01\n",
      " -5.00000000e-01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = c = -1\n",
    "b = 2\n",
    "n = 5\n",
    "K = [1, 2] # Because we are only concerned with the 2 largest\n",
    "\n",
    "lambda_k = []\n",
    "v_ik = []\n",
    "for k in range(2):\n",
    "    lambda_k.append(b + 2 * np.sqrt(a * c) * np.cos((k + 1) * np.pi / (n + 1)))\n",
    "    v = np.empty(n)\n",
    "    for i in range(n):\n",
    "        v[i] = 2 * (np.sqrt(c / a)) ** (k + 1) * np.sin((i + 1) * (k + 1) * np.pi / (n + 1))\n",
    "    v_ik.append(v / norm(v))\n",
    "\n",
    "print(\"Expected first two eigenvalues and eigenvectors:\")\n",
    "for i in range(2):\n",
    "    print(f\"Eigenvalue: {lambda_k[i]}\\nEigenvector: {v_ik[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first set of eigenvalue and eigenvector clearly matches, while second one doesn't. There are a few reasons which can perhaps explain this:\n",
    "* The Power Iteration method grows increasingly inaccurate for the subsequent eigenvalues and vectors due to error accumulation.\n",
    "* Perhaps, instead of taking a norm for the lambda updates, taking a dot product between the updates x_vectors might help decreases the error accumulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_iter(A, B, init_val, iter_lim, tol):\n",
    "    \"\"\"\n",
    "    # A: N by N np.array() | Contains coefficients of the variables (Matrix, A)\n",
    "    # B: N by 1 np.array() | Contains the constants on RHS (Vector, B)\n",
    "    # init_val: np.array() | Contains Initial Values\n",
    "    # iter_lim: Stores Iteration Limit\n",
    "    # tol: Tolerance value - how precise does the solution need to be\n",
    "    \"\"\"\n",
    "    CONV_FLAG = False # Convergence Flag\n",
    "    ITER_LIM = iter_lim\n",
    "    var = init_val # Vector, X\n",
    "    \n",
    "    for i in range(ITER_LIM):\n",
    "        var_new = np.zeros_like(var) # stores updated values of all variables (Vector, X)\n",
    "\n",
    "        for j in range(A.shape[0]):\n",
    "            # Matrix Multiplying all elements, before A's diagonal (in a row) with all corresponding vars (in Vector, X)\n",
    "            d = np.dot(A[j, :j], var[:j])\n",
    "            # Matrix Multiplying all elements, after A's diagonal (in a row) with all corresponding vars (in Vector, X)\n",
    "            r = np.dot(A[j, j + 1:], var[j + 1:])\n",
    "            # Updating values of vars\n",
    "            var_new[j] = (B[j] - d - r)/A[j, j]\n",
    "        \n",
    "        # Checks, how close the updated values are, to the previous iteration's values and breaks the loop, if close enough (defined by \"tol\")\n",
    "        if np.allclose(var, var_new, atol=tol, rtol=0.):\n",
    "            print(\"\\nSolution converged, after {} iterations.\".format(i))\n",
    "            CONV_FLAG = True\n",
    "            break\n",
    "\n",
    "        var = var_new # Storing the new solution\n",
    "    # If solution is not obtained (no convergence), after ITER_LIM iterations | Note that, this \"else\" block belongs to the previous \"for\" statement and not any \"if\" statement\n",
    "    else:\n",
    "        CONV_FLAG = False\n",
    "\n",
    "    # If Solution converged\n",
    "    if CONV_FLAG:\n",
    "        print(\"SOLUTION: \", var)\n",
    "        print(\"ERRORs: \", np.dot(A, var) - B) # Error in the obtained solution\n",
    "    else:\n",
    "        print(\"\\nSolution did not converge, after the specified limit of {} iterations.\".format(ITER_LIM))\n",
    "\n",
    "\n",
    "def gauss_seidel(A, B, init_val, iter_lim, tol):\n",
    "    \"\"\"\n",
    "    # A: N by N np.array() | Contains coefficients of the variables (Matrix, A)\n",
    "    # B: N by 1 np.array() | Contains the constants on RHS (Vector, B)\n",
    "    # init_val: np.array() | Contains Initial Values\n",
    "    # iter_lim: Stores Iteration Limit\n",
    "    # tol: Tolerance value - how precise does the solution need to be\n",
    "    \"\"\"\n",
    "    CONV_FLAG = False # Convergence Flag\n",
    "    ITER_LIM = iter_lim\n",
    "    var = init_val # Vector, X\n",
    "\n",
    "    for i in range(ITER_LIM):\n",
    "        var_new = np.zeros_like(var) # stores updated values of all variables (Vector, X)\n",
    "\n",
    "        for j in range(A.shape[0]):\n",
    "            # Matrix Multiplying all elements, before A's diagonal (in a row) with all corresponding vars (in Vector, X), that now have updated values\n",
    "            l = np.dot(A[j, :j], var_new[:j]) # Note, the only change from jacobi_iter() is changing \"var\" to \"var_new\"\n",
    "            # Matrix Multiplying all elements, after A's diagonal (in a row) with all corresponding vars (in Vector, X), that do not have updated values yet\n",
    "            u = np.dot(A[j, j + 1:], var[j + 1:])\n",
    "            # Updating values of vars\n",
    "            var_new[j] = (B[j] - l - u) / A[j, j]\n",
    "        \n",
    "        # Checks, how close the updated values are, to the previous iteration's values and breaks the loop, if close enough (defined by \"tol\")\n",
    "        if np.allclose(var, var_new, atol=tol, rtol=0.):\n",
    "            print(\"\\nSolution converged, after {} iterations.\".format(i))\n",
    "            CONV_FLAG = True\n",
    "            break\n",
    "\n",
    "        var = var_new # Storing the new solution\n",
    "    # If solution is not obtained (no convergence), after ITER_LIM iterations | Note that, this \"else\" block belongs to the previous \"for\" statement and not any \"if\" statement\n",
    "    else:\n",
    "        CONV_FLAG = False\n",
    "\n",
    "    # If Solution converged\n",
    "    if CONV_FLAG:\n",
    "        print(\"SOLUTION: \", var)\n",
    "        print(\"ERRORs: \", np.dot(A, var) - B) # Error in the obtained solution\n",
    "    else:\n",
    "        print(\"\\nSolution did not converge, after the specified limit of {} iterations.\".format(ITER_LIM))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobi Iteration:\n",
      "\n",
      "Solution converged, after 35 iterations.\n",
      "SOLUTION:  [ 1.49999955 -0.5         2.         -2.50000051  1.         -0.99999973]\n",
      "ERRORs:  [ 1.54738835e-06  0.00000000e+00  0.00000000e+00  1.73871588e-06\n",
      "  0.00000000e+00 -1.76005297e-06]\n",
      "\n",
      "Gauss Seidel:\n",
      "\n",
      "Solution converged, after 12 iterations.\n",
      "SOLUTION:  [ 1.49999932 -0.5         2.         -2.49999966  1.         -1.        ]\n",
      "ERRORs:  [ 1.02103174e-06 -5.63993297e-14  0.00000000e+00 -2.05200301e-10\n",
      "  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [-2, 0, 0, -1, 0, 0.5],\n",
    "    [0, 4, 0.5, 0, 1, 0],\n",
    "    [0, 0.5, 1.5, 0, 0, 0],\n",
    "    [-1, 0, 0, -2, 0, 1],\n",
    "    [0, 1, 0, 0, -2.5, 0],\n",
    "    [0.5, 0, 0, 1, 0, -3.75]\n",
    "])\n",
    "\n",
    "b = np.array([-1, 0, 2.75, 2.5, -3, 2])\n",
    "\n",
    "print(\"Jacobi Iteration:\")\n",
    "jacobi_iter(A, b, init_val=np.zeros(6), iter_lim=50, tol=1e-6)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Gauss Seidel:\")\n",
    "gauss_seidel(A, b, init_val=np.zeros(6), iter_lim=50, tol=1e-6)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "84c0953d57bd0dacc933a02cd955724970d5541dd435ef2a15a195c9e72ed36e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
